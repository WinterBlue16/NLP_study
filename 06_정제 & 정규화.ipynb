{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 06) 정제(Cleaning) & 정규화(Normalization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "코퍼스에서 용도에 맞게 토큰을 분류하는 작업을 토큰화(tokenization)라고 한다. 이러한 작업 전후에는 텍스트 데이터를 정제(Cleaning) 및 정규화(Normalization)하는 일이 항상 함께한다. 그 목적은 각각 다음과 같다.\n",
    "\n",
    "- 정제 : 갖고 있는 코퍼스로부터 노이즈 데이터를 제거한다.\n",
    "- 정규화 : 표현방법이 다른 언어들을 통합시켜서 같은 단어로 만들어준다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 규칙에 기반한 표기가 다른 단어들의 통합\n",
    "정규화 규칙의 한 예로서 같은 의미를 가지고 있음에도 표기가 다른 단어들을 하나의 단어로 통일해 정규화하는 방법을 사용할 수 있다. \n",
    "\n",
    "ex) USA, US/ uh-huh, uhhuh\n",
    "\n",
    "이러한 과정에서 어간 추출(stemming)과 표제어 추출(lemmatization)을 사용한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 대, 소문자 통합\n",
    "영어권에서 대문자는 문장의 맨 앞 혹은 특정한 경우에만 사용되기 때문에, 이러한 통합 작업은 보통 대문자를 소문자로 변환하는 작업으로 이루어진다.<br><br> \n",
    "다만, 대문자와 소문자를 무작정 통합해서는 안 된다. 대소문자가 명백히 구분되어야 하는 경우도 있기 때문이다. 미국을 뜻하는 단어 US와 우리를 뜻하는 단어 us가 다르듯이. 또한 고유명사에 해당하는 회사 이름(ex)Samsung)이나, 사람 이름(Kyunghee) 등은 대문자로 유지되어야 한다. <br><br>\n",
    "이러한 정규화 작업은 데이터에 따라 달라질 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 불필요한 단어 제거(Removing Unnecessary Words)\n",
    "분석하고자 하는 목적에 맞지 않는 불필요한 단어들을 노이즈 데이터라고 한다. <br><br>\n",
    "이러한 단어들을 제거하는 방법으로는 불용어 제거와 등장 빈도가 적은 단어, 길이가 짧은 단어들을 제거하는 방법이 있다. \n",
    "\n",
    "### 1) 등장 빈도가 적은 단어\n",
    "텍스트 데이터에 등장하는 빈도수가 너무 적어 자연어 처리에 도움이 되지 않는 단어들을 제거한다. \n",
    "\n",
    "### 2) 길이가 짧은 단어\n",
    "영어권 언어에서는 길이가 짧은 단어를 삭제하는 것만으로도 의미가 없는 단어들을 상당히 제거할 수 있다고 한다. 즉, 영어권에서 길이가 짧은 단어들은 대부분 불용어에 해당한다는 것이다.<br><br>\n",
    "길이가 짧은 단어를 제거하는 또 다른 중요한 이유는, 단어가 아닌 구두점들까지도 한꺼번에 제거하기 위해서이다. 하지만 한국어에서는 이러한 방식이 통하지 않을 수 있다. 그 이유는 다음과 같다. <br><br>\n",
    "첫 번째로 영어 단어의 평균 길이와 한국어 단어의 평균 길이의 차이를 들 수 있다. 일반화할 수는 없으나 한국어 단어의 길이는 영어 단어의 길이보다 짧은 편이다. 단적인 예로 영어 단어 중 가장 긴 단어로 알려진 Pneumonoultramicroscopicsilicovolcanoconiosis는 한국어로 번역하면 진폐증이다(진폐증 전체를 지칭하는 단어는 아니고 한 종류에 해당하지만). 저 긴 단어가 한국어로 세 글자면 끝이다. 다른 예로 dragon의 경우, 한국어 번역은 용이라는 한 글자로 끝이다. <br><br>\n",
    "이런 특성으로 인해 영어는 길이가 2~3인 단어를 제거하는 것만으로도 큰 의미가 없는 불용어들을 효과적으로 줄일 수 있다. 1글자인 단어를 모두 제거한다고 했을 때, 많이 쓰이긴 하지만 의미가 없는 관사 'a', 주어로 쓰이는 'I'가 모두 제거됩니다. 여기에 2글자인 단어를 제거한다고 하면, 'it', 'at', 'to', 'on' 등 대부분 불용어에 해당하는 단어들이 제거된다. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " was wondering anyone out there could enlighten this car.\n"
     ]
    }
   ],
   "source": [
    "# 길이가 1~2인 단어들을 정규표현식을 이용하여 삭제\n",
    "import re\n",
    "text = \"I was wondering if anyone out there could enlighten me on this car.\"\n",
    "shortword = re.compile(r'\\W*\\b\\w{1,2}\\b')\n",
    "print(shortword.sub('', text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 정규 표현식(Regular Expression)\n",
    "얻어낸 말뭉치에서 노이즈 데이터의 특징을 잡아낼 수 있다면, 정규 표현식을 통해 이를 제거할 수 있다. 예를 들어 웹크롤링을 통해 HTML 문서로부터 말뭉치를 가져왔다면 문서 곳곳에 자연어 처리에 필요 없는 HTML 태그들이 존재하고 있을 것이다. 정규표현식은 이러한 말뭉치 내에 계속 등장하는 글자들을 규칙에 기반하여 한 번에 제거하는 데 매우 유용하다. <br><br>\n",
    "정규식은 다른 문서에서 한꺼번에 정리하여 설명하도록 하겠다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
