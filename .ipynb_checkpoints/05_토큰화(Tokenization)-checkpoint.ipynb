{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01) 토큰화(Tokenization)\n",
    "> 자연어 처리에서 크롤링 등을 통해 얻어낸 데이터를 용도에 맞게 전처리(토큰화-정제-정규화)하는 과정 1\n",
    "\n",
    "토큰화(Tokenization)이란 주어진 코퍼스(corpus)에서 토큰(token)이라 불리는 단위로 나누는 작업이다. 토큰의 단위는 상황에 따라 다르지만, 보통 의미있는 단어들로 토큰을 정의한다. \n",
    "\n",
    "이 챕터에서는 토큰화에 대해 발생할 수 있는 여러 상황에 대해 언급하고, 토큰화에 대한 개념을 이해한다. 또한 파이썬과 NLTK 패키지, koNLPY를 통해 실습을 진행한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 단어 토큰화(Word Tokenization)\n",
    "> 토큰의 기준은 단어(Word)\n",
    "\n",
    "여기서 단어(word)란 단어 외에도 단어구, 의미를 갖는 문자열로 간주되기도 한다. 아래는 예시이다. \n",
    "\n",
    "입력 : Time is an illusion. Lunchtime double so!\n",
    "\n",
    "해당 문장에서 구두점(punctuation)과 같은 문자는 제외시키는 간단한 토큰화 작업을 진행한다. 구두점이란, 온점(.), 컴마(,), 물음표(?), 세미콜론(;), 느낌표(!) 등과 같은 기호를 말한다. \n",
    "\n",
    "결과는 다음과 같다. \n",
    "\n",
    "출력 : \"Time\", \"is\", \"an\", \"illustion\", \"Lunchtime\", \"double\", \"so\"\n",
    "\n",
    "이는 구두점을 지운 후 띄어쓰기(whitespace) 기준으로 잘라낸 결과이다. 다만 이는 매우 기초적인 예제로, 일반적인 토큰화 작업은 이와 같이 구두점이나 특수문자를 모두 제거하는 정제(cleaning) 작업만으로 끝나지 않는다. 때로는 구두점이나 특수문자를 제거했을 때 토큰이 의미를 잃어버리는 경우도 존재한다. 특히 한국어의 경우 띄어쓰기만으로 단어 토큰이 구별되지 않는 경우가 많으니 주의해야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
