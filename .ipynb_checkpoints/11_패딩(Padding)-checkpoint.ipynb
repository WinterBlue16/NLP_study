{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11_패딩(Padding)\n",
    "자연어 처리를 하다 보면 각 문장 혹은 문서의 길이가 서로 다른 경우가 있다. 기계는 길이가 동일한 문서들에 대해 하나의 행렬로 간주하고, 한꺼번에 묶어 처리(병렬연산)한다. 이는 연산의 속도와 효율성을 높일 수 있는 방법이기에, 이를 위해 여러 문장의 길이를 임의로 같게 맞추어 주는 작업이 필요하다. 실습을 통해 이해해보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Numpy로 패딩하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "정수 인코딩에서 수행했던 실습을 반복해보도록 한다. 아래와 같은 텍스트 데이터를 생성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'], ['knew', 'secret'], ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'], ['barber', 'kept', 'word'], ['barber', 'kept', 'word'], ['barber', 'kept', 'secret'], ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'], ['barber', 'went', 'huge', 'mountain']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "정수 인코딩을 수행한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sentences) # fit_on_texts() 안에 코퍼스를 입력하면 빈도수를 기준으로 단어 집합을 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이에 따라 텍스트의 모든 단어들은 빈도수의 수에 따라 순위가 매겨지게 되고, 각 단어들에는 그 순위와 같은 정수가 맵핑된다. 이 정수들을 위의 텍스트 데이터와 치환한 것이 정수 인코딩이며, 아래의 출력이다.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 5], [1, 8, 5], [1, 3, 5], [9, 2], [2, 4, 3, 2], [3, 2], [1, 4, 6], [1, 4, 6], [1, 4, 2], [7, 7, 3, 2, 10, 1, 11], [1, 12, 3, 13]]\n"
     ]
    }
   ],
   "source": [
    "encoded = tokenizer.texts_to_sequences(sentences)\n",
    "print(encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모든 단어가 고유한 정수로 변환된 것을 확인할 수 있다. 이제 각 데이터를 동일한 길이로 맞추어 주기 위해 위의 데이터 중 가장 길이가 긴 문장의 길이를 계산한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
