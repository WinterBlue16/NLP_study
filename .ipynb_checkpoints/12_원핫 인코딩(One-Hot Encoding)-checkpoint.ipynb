{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12_원핫 인코딩(One-Hot Encoding)\n",
    "컴퓨터, 혹은 기계는 문자보다 숫자를 더 잘 처리한다. 그 때문에 자연어 처리에서는 문자를 숫자로 바꾸는 여러가지 기법들이 존재한다. 원핫 인코딩(One-Hot Encoding)은 그 많은 기법 중에서 단어를 표현하는 가장 기본적인 표현방법이며, 머신러닝, 딥 러닝을 하기 위해서는 반드시 배워야 한다. \n",
    "<br>\n",
    "\n",
    "원핫 인코딩에 대해 배우기 앞서 **단어 집합(vocabulary)**에 대해서 정의해보자. 어떤 사람들은 '사전'이라고도 부르지만, 집합이라는 표현이 보다 명확해 보이므로 이하에서는 단어 집합이라 서술한다. 단어 집합은 앞으로의 자연어 처리에서도 계속 나오는 개념이므로 여기서 이해하고 넘어가야 한다. 단어 집합은 서로 다른 단어들의 집합이다. 여기서 혼동이 없도록 '서로 다른 단어'라는 정의에 대해 좀 더 주목할 필요가 있다.\n",
    "\n",
    "단어 집합(vocabulary)에서는 기본적으로 book과 books와 같이 단어의 변형 형태도 다른 단어로 간주한다. 여기서는 앞으로 단어 집합에 있는 단어들을 가지고, 문자를 숫자(더 구체적으로는 벡터)로 바꾸는 원핫 인코딩을 포함한 여러 방법에 대해 배우게 된다. \n",
    "<br>\n",
    "\n",
    "원핫 인코딩을 위해 가장 먼저 해야 할 일은 단어 집합을 만드는 것이다. 단어 집합은 텍스트의 모든 단어를 중복을 허용하지 않고 모아놓은 것이다. 그리고 이 단어 집합에 고유한 숫자를 부여하는 정수 인코딩을 진행한다. 텍스트에 단어가 총 5,000개가 존재한다면, 단어 집합의 크기는 5,000이다. 5,000개의 단어가 있는 이 단어 집합의 단어들마다 1번부터 5,000번까지 인덱스를 부여한다고 가정한다. 그럴 경우 book은 150번, dog은 171번, love는 192번, books는 212번과 같이 부여할 수 있다. \n",
    "<br>\n",
    "\n",
    "이제 각 단어에 고유한 정수 인덱스를 부여하였다고 가정한다. 이 숫자로 바뀐 단어들을 벡터로 다루고 싶다면 어떻게 해야 하는지 알아보자. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 원핫 인코딩(One-Hot Encoding) 이란? \n",
    "원핫 인코딩은 단어 집합의 크기를 벡터의 차원으로 하고, 표현하고 싶은 단어의 인덱스에는 1을, 다른 인덱스에는 0을 부여하는 표현방식이다. 이렇게 표현된 벡터를 원핫 벡터(One-Hot vector)라고 한다. \n",
    "<br>\n",
    "\n",
    "원핫 인코딩을 두 가지 과정으로 정리해본다.\n",
    "\n",
    "1. 각 단어에 고유한 인덱스를 부여한다(정수 인코딩)\n",
    "2. 표현하고 싶은 단어 인덱스의 위치에 1을, 다른 단어의 인덱스 위치에는 0을 부여한다. \n",
    "\n",
    "이해를 돕기 위해 다음의 한국어 문장을 예제로 원핫 벡터를 만들어보도록 한다. \n",
    "<br>\n",
    "<br>\n",
    "\n",
    "\n",
    "**문장 : 나는 자연어 처리를 배운다**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['나', '는', '자연어', '처리', '를', '배운다']\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "okt = Okt()\n",
    "token = okt.morphs(\"나는 자연어 처리를 배운다\")\n",
    "print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "konlpy의 Okt 형태소 분석기를 통해 우선 문장에 대한 토큰화를 수행하였다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'나': 0, '는': 1, '자연어': 2, '처리': 3, '를': 4, '배운다': 5}\n"
     ]
    }
   ],
   "source": [
    "word2index = {}\n",
    "for voca in token:\n",
    "    if voca not in word2index.keys():\n",
    "        word2index[voca] = len(word2index)\n",
    "\n",
    "print(word2index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각 토큰에 대해서 고유한 인덱스(index)를 부여했다. 지금은 문장이 짧으므로 각 단어의 빈도수를 고려하지 않지만, 빈도수 순대로 단어를 정렬하여 고유한 인덱스를 부여하는 작업이 사용되기도 한다.([정수 인코딩 문서]() 참고) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding(word, word2index):\n",
    "    one_hot_vector = [0]*(len(word2index))\n",
    "    index = word2index[word]\n",
    "    one_hot_vector[index]=1\n",
    "    return one_hot_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "토큰을 입력하면 해당 토큰에 대한 원핫 벡터를 생성하는 함수를 만들었다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 1, 0, 0, 0]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_encoding(\"자연어\", word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 1]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_encoding(\"배운다\", word2index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "해당 함수에 '자연어', '배운다'라는 토큰을 입력으로 넣어봤더니 각각 [0,0,1,0,0,0], [0,0,0,0,0,1]라는 벡터가 나왔다. 자연어는 단어 집합 인덱스가 2이므로, 자연어를 표현하는 원핫벡터는 인덱스 2의 값이 1이 되고 나머지 값은 0이 된다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 케라스(Keras)를 이용한 원핫 인코딩(One-Hot Encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위에서는 원핫 인코딩을 이해하기 위해 파이썬으로 직접 코드를 작성하였다. 하지만 케라스에서는 원핫 인코딩을 수행하는 유용한 도구인 to_categorical()을 지원한다. 이번에는 케라스만으로 정수 인코딩과 원핫 인코딩을 순차적으로 진행해 본다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"나랑 점심 먹으로 갈래 점심 메뉴는 햄버거 갈래 갈래 햄버거 최고야\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위와 같은 문장이 있다고 했을 때, 정수 인코딩 챕터에서와 같이 케라스 토크나이저를 이용한 정수 인코딩은 다음과 같다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'갈래': 1, '점심': 2, '햄버거': 3, '나랑': 4, '먹으러': 5, '메뉴는': 6, '최고야': 7}\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "text = \"나랑 점심 먹으러 갈래 점심 메뉴는 햄버거 갈래 갈래 햄버거 최고야\"\n",
    "\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts([text])\n",
    "print(t.word_index) # 각 단어에 대한 인코딩 결과 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위와 같이 생성된 단어 집합(vocabulary)에 있는 단어들로만 구성된 텍스트가 있다면 texts_to_sequences()를 통해서 이를 정수 시퀸스로 변환가능하다. 생성된 단어 집합 내의 일부 단어들로만 구성된 서브 텍스트 sub_text를 만들어 확인해보자. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 5, 1, 6, 3, 7]\n"
     ]
    }
   ],
   "source": [
    "sub_text = \"점심 먹으러 갈래 메뉴는 햄버거 최고야\"\n",
    "encoded = t.texts_to_sequences([sub_text])[0]\n",
    "print(encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "지금까지 진행한 것은 이미 정수 인코딩 문서에서 배운 내용이다. 이제 해당 결과를 가지고, 원핫 인코딩을 진행해보자. 케라스는 정수 인코딩 된 결과로부터 원핫 인코딩을 수행하는 to_categorical()을 지원한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "one_hot = to_categorical(encoded)\n",
    "print(one_hot) # 각 인덱스의 원핫 벡터"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 결과는 \"점심 먹으러 갈래 메뉴는 햄버거 최고야\"라는 문장이 [2,5,1,6,3,7]로 정수 인코딩이 되고 나서, 각각의 인코딩된 결과를 인덱스로 해 원핫 인코딩이 수행된 모습을 보여준다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 원핫 인코딩의 한계"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이런 표현 방식의 단점은 단어의 개수가 늘어날수록, 벡터를 저장하기 위해 필요한 공간이 계속 늘어날 수밖에 없다는 것이다. 다른 말로는 벡터의 차원이 계속 늘어난다, 고 표현할 수 있다. 원핫 벡터는 단어 집합의 크기가 곧 벡터 차원의 수가 됩니다. 가령, 단어가 1,000개인 코퍼스를 가지고 원핫 벡터를 만들면, 모든 단어 각각은 모두 1,000개의 차원을 가진 벡터가 된다. 다시 말해, 모든 단어 각각은 하나의 값만 1을 가지고, 나머지 999개의 값은 0을 가지는 벡터가 된다는 거다.<br>\n",
    "\n",
    "또한 원핫 벡터는 단어이 유사도를 표현하지 못한다는 단점이 있다. 예를 들어 늑대, 호랑이, 강아지, 고양이라는 4개의 단어에 대해 원핫 인코딩을 해서 각각 [1,0,0,0], [0,1,0,0], [0,0,1,0], [0,0,0,1]이라는 원핫 벡터를 부여받았다고 하자. 이 방법으로는 강아지와 늑대가 유사하고, 호랑이와 고양이가 유사하다는 것을 나타낼 방법이 없다. 좀 더 극단적인 예시를 들어보면, 강아지, 개, 냉장고라는 단어가 있을 때 강아지라는 단어가 개와 냉장고라는 단어 중 어떤 단어와 더 유사한지도 알 수 없다.<br>\n",
    "\n",
    "단어 간 유사성을 알 수 없다는 단점은 검색 시스템에서 심각한 문제이다. 가령, 여행을 가려고 웹 검색창에 '삿포로 숙소'라는 단어를 검색한다고 하자. 제대로 된 검색 시스템이라면, '삿포로 숙소'라는 검색어에 대해서 '삿포로 게스트하우스', '삿포로 료칸', '삿포로 호텔'과 같은 유사 단어에 대한 결과도 함께 보여줄 수 있어야 한다. 하지만 단어 간 유사성을 계산할 수 없다면, '게스트하우스'나 '료칸'이나 '호텔'과 같은 연관 검색어를 보여줄 수 없다.\n",
    "<br>\n",
    "\n",
    "이러한 단점을 해결하기 위해 단어의 잠재 의미를 반영하여 다차원 공간에 벡터화 하는 기법으로 크게 두 가지가 있다. <br>\n",
    "\n",
    "첫 번째는 카운트 기반의 벡터화 방법인 LSA, HAL 등이다. 두 번째는 예측 기반으로 벡터화하는 NNLM, RNNLM, Word2Vec, FastText 등이다. 카운트 기법과 예측 기반 두 가지 방법을 모두 사용하는 GloVe라는 방법도 있다. <br>\n",
    "\n",
    "여기서는 LSA를 먼저 다룰 예정이며, 그 후에 Word2Vec, FastText, GloVe를 다룬다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
