# 19_혼란도(Perplexity)

두 개의 모델 A, B가 있을 때 이 모델의 성능은 어떻게 비교할 수 있을까? 일단은 두 개의 모델을 모두 오타 교정, 기계 번역 등에 사용해보고 해당 업무의 성능을 비교함으로써 평가를 해볼 수 있다. 그러나 성능 비교를 위해 일일히 실제 작업을 시켜보고 정확도를 비교하는 것은 번거롭고, 효율이 떨어지는 일이다. 단적인 예로, 비교해야 하는 모델이 두 개가 아니라 그 이상의 수라면 작업 시간은 측정하기 힘들 정도로 불어날 것이다. <br>

이러한 방식을 외부 평가(extrinsic evaluation)라고 하는데, 이러한 평가보다는 어쩌면 조금 부정확할 수도 있으나 테스트 데이터에 대해서 빠르게 결과를 도출하는 더 간단한 평가 방법이 있다. 바로 모델 내에서 성능을 수치화하여 결과를 내놓는 내부 평가(Intrinsic evaluation)에 해당하는 펄플렉서티(perplexity)이다. 

## 1. 언어 모델의 평가 방법(Evaluation metric) : PPL

펄플렉서티(perplexity), 줄여서 PPL은 언어 모델을 평가하기 위한 내부 평가 지표이다. 조금 근본적인 질문을 하나 던져보자. 왜 perplexity라는 용어를 사용했을까? 영어에서 'perplexed'는 '헷갈리는'과 유사한 의미를 가진다. 그러니 여기서 PPL은 '헷갈리는 정도', '혼란도' 정도로 이해하자. PPL을 처음 배울 때 다소 낯설게 느껴질 수 있는 점이 있다면, PPL은 수치가 높을수록 성능이 좋은 것이 아니라, **수치가 낮을수록** 성능이 좋다는 것을 의미한다는 것이다. <BR>



PPL은 단어의 수로 정규화(normalization)된 테스트 데이터에 대한 확률의 역수이다. PPL을 최소화한다는 것은 문장의 확률을 최대화하는 것과 같다. 문장 W의 길이가 N이라고 하였을 때, PPL은 다음과 같다. 

![수식](https://user-images.githubusercontent.com/58945760/104689013-2466aa00-5745-11eb-9d26-08e6be43649d.PNG)

문장 확률에 체인 룰(chain rule)을 적용하면 아래와 같다. 여기서 체인 룰이란 쉽게 말해 복잡한 함수(보통 두 개 이상의 함수가 결합된 함수)의 미분방법이다. 쉽지 않은 개념이므로 일단은 언급만 하고 넘어간다. 

![수식2](https://user-images.githubusercontent.com/58945760/104689020-26306d80-5745-11eb-942b-ec8cc2dc0cc2.PNG)

여기에 n-gram을 적용해볼 수도 있다. 예를 들어, bigram 언어 모델의 경우 식이 아래와 같아진다. 
$$
PPL(W)=\sqrt[N]{\frac {1}{\prod_{i=1}^NP(w_i|w_{i-1})}}
$$

## 2. 분기 계수(Branching factor)

PPL은 선택 가능한 경우의 수를 의미하는 '분기계수(branching factor)'이기도 하다. PPL은 이 언어 모델이 특정 시점에서 평균적으로 몇 개의 선택지를 가지고 고민하고 있는지를 알려준다. 가령, 언어 모델에 어떤 테스트 데이터를 주고 측정했더니 PPL이 10이 나왔다고 하자. 그렇다면 해당 언어 모델은 테스트 데이터에 대해서 다음 단어를 예측하는 모든 시점(time-step)마다 평균적으로 10개의 단어를 가지고 어떤 것이 정답인지 고민하고 있다는 뜻이다. 같은 테스트 데이터에 대해서 서로 다른 언어모델의 PPL을 계산해 수치를 비교해보면, 어떤 모델의 성능이 더 좋은지 판단할 수 있다. 위에서도 말했지만, 이 경우 당연히 PPL이 더 낮은 언어 모델의 성능이 더 좋다고 본다. 

![수식3](https://user-images.githubusercontent.com/58945760/104690829-7bba4980-5748-11eb-8afa-fc895c9317c8.PNG)

단, 평가 방법에 있어서 주의할 점은 PPL의 값이 낮다는 것은 어디까지나 테스트 데이터 상에서 높은 정확도를 보인다는 것이지, 사람이 직접 느끼기에 좋은 언어 모델이라는 것을 의미하지는 않는다는 점이다. 또한 언어 모델의 PPL은 테스트 데이터에 의존하므로 두 개 이상의 언어 모델을 비교할 때는 정량적으로 양이 많으며 도메인에 적합한, 동일한 테스트 데이터를 사용해야 신뢰도가 높아진다는 것도 기억해야 한다.



## 3. 기존 언어 모델 VS 인공 신경망을 이용한 언어 모델

PPL의 실제 사용 사례를 확인해보자. 페이스북 AI 연구팀은 우리가 앞서 배운 n-gram 언어 모델과 이후 배우게 될 딥러닝을 활용한 언어 모델에 대해 PPL을 기준으로 성능 테스트를 한 표를 공개한 바 있다. 

![img](https://wikidocs.net/images/page/21697/ppl.PNG)

이름에서 알 수 있듯, 표에서 맨 윗줄에 있는 모델이 n-gram을 이용한 것이며 PPL이 67.6으로 측정되었다. 5-gram을 사용했고, 5-gram 앞에 Interpolated Kneser-Ney라는 이름이 붙어있는데 여기서는 별도의 설명을 생략하겠다고 했던 일반화(generalization) 방법이 사용된 모델이다. 반면, 그 아래의 모델들은 인공 신경망을 이용한 언어 모델들로 페이스북 AI 연구팀이 자신들의 언어 모델을 다른 언어 모델과 비교하고자 하는 목적으로 기록하였다. 아직 RNN과 LSTM 등이 무엇인지 다루지는 않았으나, 인공 신경망을 이용한 언어 모델들이 대부분 n-gram을 이용한 모델보다 더 좋은 성능으로 평가받았음을 확인할 수 있다.  